{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, requests\n",
    "\n",
    "durl = \"https://www.cnn.com/2023/02/04/uk/nicola-bulley-missing-intl-gbr/index.html\"\n",
    "\n",
    "df = requests.get(durl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2310454"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(type(df))\n",
    "\n",
    "# open(\"data/cnn_article.txt\", 'wb').write(df.content)\n",
    "\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fl = open(\"data/cnn_article.txt\")\n",
    "cntnt1 = fl.read()\n",
    "cntnt = cntnt1[1:500]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "type(stop_words)\n",
    "print(stop_words.__len__())\n",
    "\n",
    "sstop_words = set(stopwords.words(\"english\"))\n",
    "type(sstop_words)\n",
    "print(sstop_words.__len__())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rlist(list):\n",
    "    __clist = []\n",
    "    # def __init__(initList):\n",
    "    #     this.__clist = initList\n",
    "\n",
    "    def __init__(self,initList=[]):\n",
    "        self.__clist = initList\n",
    "        super().__init__(initList)\n",
    "        \n",
    "    # def __sub__(self,obj:list):\n",
    "    #     # impl rlist - list\n",
    "    #     retlist  = []\n",
    "    #     for item in self.__clist: # rlist \n",
    "    #         if item in obj: # check if rlist item exists in list\n",
    "    #             retlist.append(item)\n",
    "    #     return retlist\n",
    "    def __sub__(self,obj):\n",
    "        # impl rlist - list\n",
    "        retlist  = []\n",
    "        if obj is not None:\n",
    "            for item in self: # elist \n",
    "                if item not in obj: # check if rlist item exists in list\n",
    "                    retlist.append(item)\n",
    "        else:\n",
    "            print(\"Cant work with None\")\n",
    "            retlist = []\n",
    "        return retlist\n",
    "\n",
    "    def len(self):\n",
    "        # return self.__clist.__len__()\n",
    "        return len(self)\n",
    "    \n",
    "    def append(self,xlist):\n",
    "        # op = rlist(self)\n",
    "        for elem in xlist:\n",
    "            self.append(elem)\n",
    "        # return \n",
    "    \n",
    "    # def sort(self):\n",
    "    #     return sort(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.rlist'>\n",
      "['a', 'b', 'c']\n",
      "3\n",
      "<class '__main__.rlist'>\n",
      "['a', 'c']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "a1 = ['a','b','c']\n",
    "a = rlist(a1)\n",
    "print(type(a))\n",
    "print(a)\n",
    "print(a.len())\n",
    "\n",
    "b = rlist(['a','c'])\n",
    "print(type(b))\n",
    "print(b)\n",
    "\n",
    "c =  b -a \n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443\n",
      "tokenized words 1443\n",
      "stop words = 179\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstop words = \u001b[39m\u001b[39m{\u001b[39;00mstop_words\u001b[39m.\u001b[39mlen()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m rsw \u001b[39m=\u001b[39m rlist(stop_words)\n\u001b[0;32m---> 13\u001b[0m rsw\u001b[39m.\u001b[39;49mappend([\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     14\u001b[0m     \u001b[39m# rsw.append(x)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# print(rsw)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# rsw1 = rsw.append(stop_words)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[39m# print(f\"---{rsw1}---\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(rsw))\n",
      "Cell \u001b[0;32mIn[29], line 36\u001b[0m, in \u001b[0;36mrlist.append\u001b[0;34m(self, xlist)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(\u001b[39mself\u001b[39m,xlist):\n\u001b[1;32m     34\u001b[0m     \u001b[39m# op = rlist(self)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m xlist:\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mappend(elem)\n",
      "Cell \u001b[0;32mIn[29], line 36\u001b[0m, in \u001b[0;36mrlist.append\u001b[0;34m(self, xlist)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(\u001b[39mself\u001b[39m,xlist):\n\u001b[1;32m     34\u001b[0m     \u001b[39m# op = rlist(self)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m xlist:\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mappend(elem)\n",
      "    \u001b[0;31m[... skipping similar frames: rlist.append at line 36 (2971 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[29], line 36\u001b[0m, in \u001b[0;36mrlist.append\u001b[0;34m(self, xlist)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(\u001b[39mself\u001b[39m,xlist):\n\u001b[1;32m     34\u001b[0m     \u001b[39m# op = rlist(self)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m xlist:\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mappend(elem)\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tc = rlist(word_tokenize(cntnt))\n",
    "print(tc.len())\n",
    "\n",
    "print(f\"tokenized words {tc.len()}\")\n",
    "# print(tc)\n",
    "stop_words = rlist(stopwords.words(\"english\"))\n",
    "print(f\"stop words = {stop_words.len()}\")\n",
    "\n",
    "rsw = rlist(stop_words)\n",
    "rsw.append(['a',',','.','-'])\n",
    "    # rsw.append(x)\n",
    "    # print(rsw)\n",
    "\n",
    "# rsw1 = rsw.append(stop_words)\n",
    "\n",
    "# print(f\"---{rsw1}---\")\n",
    "print(type(rsw))\n",
    "# print(rsw.sort())\n",
    "\n",
    "print(f\"rsw stop words = {rsw.len()}\")\n",
    "print(f\"--rsw stop words = {len(rsw)}\")\n",
    "\n",
    "# print(sort(rsw))\n",
    "\n",
    "# print(tc.__len__())\n",
    "filteredw = rlist(tc - rsw)\n",
    "print(filteredw.len())\n",
    "print(filteredw)\n",
    "# filteredw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnart = open(\"data/cnn_article.txt\")\n",
    "x1 = cnnart.read()\n",
    "x = x1[1:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({' ': 80, 'e': 51, 'o': 34, 'r': 33, 'a': 31, 'n': 27, 'l': 24, 'i': 18, 't': 18, 'h': 18, 'd': 17, 'g': 15, 's': 12, 'c': 11, 'u': 11, '\\n': 9, 'f': 9, 'm': 8, 'k': 8, 'y': 7, 'p': 7, '.': 7, ',': 6, 'w': 6, 'b': 5, 'N': 3, 'v': 3, 'B': 3, '-': 2, 'C': 1, '—': 1, 'O': 1, 'J': 1, 'M': 1, 'R': 1, 'W': 1, 'L': 1, 'E': 1, 'D': 1, '9': 1, '3': 1, '0': 1, 'T': 1, '–': 1, 'A': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cw = Counter(x)\n",
    "print(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokenized words 95\n",
      "----\n",
      "--->removing she\n",
      "--->removing her\n",
      "--->removing their\n",
      "--->removing had\n",
      "--->removing a\n",
      "--->removing the\n",
      "--->removing and\n",
      "--->removing but\n",
      "--->removing as\n",
      "--->removing at\n",
      "--->removing by\n",
      "--->removing for\n",
      "--->removing into\n",
      "--->removing before\n",
      "--->removing in\n",
      "--->removing on\n",
      "--->removing off\n",
      "--->removing .\n",
      "--->removing ,\n",
      "--->removing —\n",
      "--->removing ,\n",
      "after cleansing\n",
      "74\n",
      "['ondon', 'CNN', 'On', 'cold', 'January', 'morning', 'a', 'mother-of-two', 'dropped', 'young', 'daughters', 'school', 'taking', 'her', 'springer', 'spaniel', 'usual', 'walk', 'river', 'Mortgage', 'broker', 'Nicola', 'Bulley', 'greeted', 'fellow', 'dog', 'walkers', 'took', 'a', 'route', 'along', 'the', 'River', 'Wyre', 'Lancashire', 'northern', 'England', ',', 'police', 'believe', '.', 'During', 'the', 'walk', ',', 'she', 'dialed', 'a', 'work', 'conference', 'call', ',', 'keeping', 'her', 'camera', 'off', 'microphone', 'muted', '.', 'By', '9.30', 'a.m.', ',', 'the', 'Teams', 'call', 'ended', '–', 'Bulley', 'remained', 'logged', '.', 'A', 'shor']\n"
     ]
    }
   ],
   "source": [
    "tw = word_tokenize(x1[1:500])\n",
    "print(f\"total tokenized words {len(tw)}\")\n",
    "sw = list(stop_words)\n",
    "print('----')\n",
    "\n",
    "sw1 = sw + ['-','=','.',',','—',','] \n",
    "# for x in ['-','=','.',',','—',',']: \n",
    "#     sw.append(x)\n",
    "# print(sw)\n",
    "# print(sw1)\n",
    "for w in sw1: # check for every word in stop words\n",
    "    # print(f\"chekcing for {w}\")\n",
    "    if w in tw: # if word found in stop words, remove it from main words list\n",
    "        print(f\"--->removing {w}\")\n",
    "        tw.remove(w)\n",
    "\n",
    "print(\"after cleansing\")\n",
    "print(len(tw))\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ondon', 'CNN', '—', 'On', 'cold', 'January', 'morning', ',', 'a', 'mother-of-two', 'dropped', 'young', 'daughters', 'school', 'taking', 'her', 'springer', 'spaniel', 'usual', 'walk', 'river', '.', 'Mortgage', 'broker', 'Nicola', 'Bulley', 'greeted', 'fellow', 'dog', 'walkers', 'took', 'a', 'route', 'along', 'the', 'River', 'Wyre', 'Lancashire', ',', 'northern', 'England', ',', 'police', 'believe', '.', 'During', 'the', 'walk', ',', 'she', 'dialed', 'a', 'work', 'conference', 'call', ',', 'keeping', 'her', 'camera', 'off', 'microphone', 'muted', '.', 'By', '9.30', 'a.m.', ',', 'the', 'Teams', 'call', 'ended', '–', 'Bulley', 'remained', 'logged', '.', 'A', 'shor']\n",
      "======\n",
      "['ondon', 'CNN', '—', 'On', 'cold', 'Januari', 'morn', ',', 'a', 'mother-of-two', 'drop', 'young', 'daughter', 'school', 'take', 'her', 'spring', 'spaniel', 'usual', 'walk', 'river', '.', 'Mortgag', 'broker', 'Nicola', 'Bulley', 'greet', 'fellow', 'dog', 'walker', 'took', 'a', 'rout', 'along', 'th', 'River', 'Wyre', 'Lancashir', ',', 'northern', 'England', ',', 'polic', 'believ', '.', 'Dure', 'the', 'walk', ',', 'she', 'dial', 'a', 'work', 'confer', 'call', ',', 'keep', 'her', 'camera', 'off', 'microphon', 'mute', '.', 'By', '9.30', 'a.m.', ',', 'th', 'Team', 'call', 'end', '–', 'Bulley', 'remain', 'log', '.', 'A', 'shor']\n"
     ]
    }
   ],
   "source": [
    "from porter2stemmer import Porter2Stemmer \n",
    "# for w in tw:\n",
    "#     p2s.stem(w)\n",
    "\n",
    "stemmer = Porter2Stemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tw]\n",
    "print(tw)\n",
    "print(\"======\")\n",
    "print(stemmed_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Jan 11 2023, 14:15:54) [GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
